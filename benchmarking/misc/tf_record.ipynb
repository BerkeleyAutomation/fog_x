{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring read and write speed of TFrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "read_files = [\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00000-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00001-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00002-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00003-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00004-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00005-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00006-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00007-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00008-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00009-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00010-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00011-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00012-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00013-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00014-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00015-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00016-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00017-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00018-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00019-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00020-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00021-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00022-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00023-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00024-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00025-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00026-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00027-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00028-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00029-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00030-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00031-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00032-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00033-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00034-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00035-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00036-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00037-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00038-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00039-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00040-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00041-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00042-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00043-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00044-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00045-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00046-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00047-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00048-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00049-of-00050'\n",
    "]\n",
    "\n",
    "write_file = '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test_write_test.tfrecord'\n",
    "\n",
    "def measure_read_speed(files):\n",
    "    start_time = time.time()\n",
    "    total_bytes = 0\n",
    "\n",
    "    for file_path in files:\n",
    "        for record in tf.data.TFRecordDataset(file_path):\n",
    "            # Measure the speed of converting the record to numpy + for loop time\n",
    "            record.numpy()\n",
    "            total_bytes += len(record.numpy())\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    read_speed = total_bytes / elapsed_time / (1024 * 1024)\n",
    "\n",
    "    print(f\"Read {total_bytes} bytes in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Read Speed: {read_speed:.2f} MB/s\")\n",
    "    print(f\"Read Latency: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "def measure_write_speed(files, output_file):\n",
    "    start_time = time.time()\n",
    "    total_bytes = 0\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_file) as writer:\n",
    "        for file_path in files:\n",
    "            for record in tf.data.TFRecordDataset(file_path):\n",
    "                writer.write(record.numpy())\n",
    "                total_bytes += len(record.numpy())\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    write_speed = total_bytes / elapsed_time / (1024 * 1024) \n",
    "\n",
    "    print(f\"Wrote {total_bytes} bytes in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Write Speed: {write_speed:.2f} MB/s\")\n",
    "    print(f\"Write Latency: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "print(\"Measuring Read Speed...\")\n",
    "measure_read_speed(read_files)\n",
    "print(\"Measuring Write Speed...\")\n",
    "measure_write_speed(read_files, write_file)\n",
    "os.remove(write_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from PIL import Image\n",
    "from IPython.display import display as ip_display\n",
    "import time\n",
    "import os\n",
    "\n",
    "exit()\n",
    "\n",
    "read_files = [\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00000-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00001-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00002-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00003-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00004-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00005-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00006-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00007-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00008-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00009-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00010-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00011-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00012-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00013-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00014-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00015-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00016-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00017-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00018-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00019-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00020-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00021-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00022-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00023-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00024-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00025-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00026-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00027-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00028-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00029-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00030-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00031-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00032-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00033-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00034-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00035-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00036-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00037-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00038-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00039-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00040-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00041-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00042-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00043-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00044-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00045-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00046-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00047-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00048-of-00050',\n",
    "    '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test.tfrecord-00049-of-00050'\n",
    "]\n",
    "\n",
    "write_file = '/home/davidh/datasets/berkeley_autolab_ur5/berkeley_autolab_ur5-test_write_test.tfrecord'\n",
    "\n",
    "def dataset2path(dataset_name):\n",
    "    if dataset_name == 'robo_net':\n",
    "        version = '1.0.0'\n",
    "    elif dataset_name == 'language_table':\n",
    "        version = '0.0.1'\n",
    "    else:\n",
    "        version = '0.1.0'\n",
    "    return f'gs://gresearch/robotics/{dataset_name}/{version}'\n",
    "\n",
    "def measure_specifying_path_latency(files):\n",
    "    start_time = time.time()\n",
    "    for file_path in files:\n",
    "        _ = file_path\n",
    "    end_time = time.time()\n",
    "    latency = end_time - start_time\n",
    "    return latency\n",
    "\n",
    "def measure_getting_image_latency_and_display(dataset_name):\n",
    "    dataset_path = dataset2path(dataset_name)\n",
    "    b = tfds.builder_from_directory(builder_dir=dataset_path)\n",
    "    if 'image' not in b.info.features['steps']['observation']:\n",
    "        raise ValueError(\n",
    "            f\"The key 'image' was not found in this dataset.\\n\"\n",
    "            \"Please choose a different image key to display for this dataset.\\n\"\n",
    "            \"Here is the observation spec:\\n\"\n",
    "            + str(b.info.features['steps']['observation']))\n",
    "\n",
    "    start_time = time.time()\n",
    "    ds = b.as_dataset(split='train[:1]').shuffle(1)  # take only the first episode\n",
    "    episode = next(iter(ds))\n",
    "    images = [step['observation']['image'] for step in episode['steps']]\n",
    "    images = [Image.fromarray(image.numpy()) for image in images]\n",
    "    end_time = time.time()\n",
    "    latency = end_time - start_time\n",
    "\n",
    "    # Display some first image\n",
    "    ip_display(images[0])\n",
    "\n",
    "    return latency\n",
    "\n",
    "def measure_read_speed(files):\n",
    "    start_time = time.time()\n",
    "    total_bytes = 0\n",
    "    total_trajectories = 0\n",
    "\n",
    "    for file_path in files:\n",
    "        for record in tf.data.TFRecordDataset(file_path):\n",
    "            record.numpy()\n",
    "            total_bytes += len(record.numpy())\n",
    "            total_trajectories += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    read_speed = total_bytes / elapsed_time / (1024 * 1024)  # conversion to mb/s\n",
    "    read_throughput = total_trajectories / elapsed_time  # trajectories/second\n",
    "\n",
    "    print(f\"Read {total_bytes} bytes in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Read Speed: {read_speed:.2f} MB/s\")\n",
    "    print(f\"Read Latency: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Read Throughput: {read_throughput:.2f} trajectories/second\")\n",
    "\n",
    "def measure_write_speed(files, output_file):\n",
    "    start_time = time.time()\n",
    "    total_bytes = 0\n",
    "    total_trajectories = 0\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_file) as writer:\n",
    "        for file_path in files:\n",
    "            for record in tf.data.TFRecordDataset(file_path):\n",
    "                writer.write(record.numpy())\n",
    "                total_bytes += len(record.numpy())\n",
    "                total_trajectories += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    write_speed = total_bytes / elapsed_time / (1024 * 1024)  # conversion to MB/S\n",
    "    write_throughput = total_trajectories / elapsed_time  # conversion to trajectories/second\n",
    "\n",
    "    print(f\"Wrote {total_bytes} bytes in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Write Speed: {write_speed:.2f} MB/s\")\n",
    "    print(f\"Write Latency: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Write Throughput: {write_throughput:.2f} trajectories/second\")\n",
    "\n",
    "print(\"Measuring Specifying Path Latency...\")\n",
    "path_latency = measure_specifying_path_latency(read_files)\n",
    "print(f\"Specifying Path Latency: {path_latency:.4f} seconds\")\n",
    "\n",
    "print(\"Measuring Getting Image Latency and Displaying Image...\")\n",
    "image_latency = measure_getting_image_latency_and_display('berkeley_autolab_ur5')\n",
    "print(f\"Getting Image Latency: {image_latency:.4f} seconds\")\n",
    "\n",
    "print(f\"Actual Image Fetch Latency: {image_latency - path_latency:.4f} seconds\")\n",
    "\n",
    "print(\"Measuring Read Speed...\")\n",
    "measure_read_speed(read_files)\n",
    "print(\"Measuring Write Speed...\")\n",
    "measure_write_speed(read_files, write_file)\n",
    "\n",
    "os.remove(write_file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
