{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "608ae1bd-0b68-4e0e-b748-e7f89807724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fog_x \n",
    "import os\n",
    "from logging import getLogger\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "import tensorflow as tf\n",
    "\n",
    "class BaseLoader():\n",
    "    def __init__(self, data_path):\n",
    "        super(BaseLoader, self).__init__()\n",
    "        self.data_dir = data_path\n",
    "        self.logger = getLogger(__name__)\n",
    "        self.index = 0\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __iter___(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseExporter():\n",
    "    def __init__(self):\n",
    "        super(BaseExporter, self).__init__()\n",
    "        self.logger = getLogger(__name__)\n",
    "\n",
    "    def export(self, loader: BaseLoader, output_path: str):\n",
    "        raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6f0124-e66c-4529-8354-12fbc0769de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTXLoader(BaseLoader):\n",
    "    def __init__(self, data_path, split):\n",
    "        super(RTXLoader, self).__init__(data_path)\n",
    "        import tensorflow_datasets as tfds\n",
    "\n",
    "        builder = tfds.builder_from_directory(data_path)\n",
    "\n",
    "        self.ds = builder.as_dataset(split=split)\n",
    "        # https://www.determined.ai/blog/tf-dataset-the-bad-parts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.index < len(self.ds):\n",
    "            self.index += 1\n",
    "            nested_dataset = self.ds.__iter__()\n",
    "            trajectory = list(nested_dataset)[0][\"steps\"]\n",
    "            ret = []\n",
    "            # Iterate through the outer dataset\n",
    "            for step_data in trajectory:\n",
    "                step = {}\n",
    "                for dataset_key, element in step_data.items():\n",
    "                    # print(np.array(element))\n",
    "                    if dataset_key == \"observation\":\n",
    "                        step[\"observation\"] = {}\n",
    "                        for obs_key, obs_element in element.items():\n",
    "                            step[\"observation\"][obs_key] = np.array(obs_element)\n",
    "                    elif dataset_key == \"action\":\n",
    "                        step[\"action\"] = {}\n",
    "                        for action_key, action_element in element.items():\n",
    "                            step[\"action\"][action_key] = np.array(action_element)\n",
    "                    else:\n",
    "                        step[dataset_key] = np.array(element)\n",
    "                ret.append(step)\n",
    "            return ret\n",
    "        else:\n",
    "            raise StopIteration\n",
    "    \n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "class RTXExporter(BaseExporter):\n",
    "    def __init__(self):\n",
    "        super(RTXExporter, self).__init__()\n",
    "\n",
    "    def export(self, loader: BaseLoader, output_path: str):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9920acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import pickle \n",
    "\n",
    "class MKVLoader(BaseLoader):\n",
    "    def __init__(self, data_path):\n",
    "        super(MKVLoader, self).__init__(data_path)\n",
    "        self.files = [data_path + f for f in os.listdir(data_path) if f.endswith('.mkv')]\n",
    "        self.index = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index < len(self):\n",
    "            result = self.files[self.index]\n",
    "            self.index += 1\n",
    "            return self._parse_mkv_file(result)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "    \n",
    "    def _parse_mkv_file(self, filename):\n",
    "        print(filename)\n",
    "        input_container = av.open(filename)\n",
    "        video_stream1 = input_container.streams.video[0] \n",
    "        video_stream1.thread_type = 'AUTO'\n",
    "        video_stream2 = input_container.streams.video[1] \n",
    "        video_stream2.thread_type = 'AUTO'\n",
    "        depth_stream = input_container.streams.video[2] \n",
    "        depth_stream.thread_type = 'AUTO'\n",
    "        data_stream = input_container.streams[3] \n",
    "\n",
    "        decoded_stream_1 = []\n",
    "        decoded_stream_2 = []\n",
    "        decoded_stream_depth = []\n",
    "        decoded_stream_data = []\n",
    "\n",
    "        pkt_counter = 0\n",
    "        for packet in input_container.demux(video_stream1, video_stream2, depth_stream, data_stream):\n",
    "            pkt_counter += 1\n",
    "            if packet.stream.index == video_stream1.index: \n",
    "                frame = packet.decode()\n",
    "                if frame:\n",
    "                    for f in frame:\n",
    "                        image = f.to_ndarray(format='rgb24')\n",
    "                        decoded_stream_1.append(image)\n",
    "            elif packet.stream.index == video_stream2.index:\n",
    "                frame = packet.decode()\n",
    "                if frame:\n",
    "                    for f in frame:\n",
    "                        image = f.to_ndarray(format='rgb24')\n",
    "                        decoded_stream_2.append(image)\n",
    "            elif packet.stream.index == depth_stream.index:\n",
    "                frame = packet.decode()\n",
    "                if frame:\n",
    "                    for f in frame:\n",
    "                        image = f.to_ndarray(format='gray')\n",
    "                        decoded_stream_depth.append(image)\n",
    "            elif packet.stream.index == data_stream.index:\n",
    "                packet_in_bytes = bytes(packet)\n",
    "                if packet_in_bytes:\n",
    "                    non_dict = pickle.loads(packet_in_bytes)\n",
    "                    decoded_stream_data.append(non_dict)\n",
    "            else:\n",
    "                print(\"Unknown stream\")\n",
    "        print(pkt_counter, len(decoded_stream_1), len(decoded_stream_2), len(decoded_stream_depth), len(decoded_stream_data))\n",
    "        input_container.close()\n",
    "\n",
    "        \n",
    "\n",
    "class MKVExporter(BaseExporter):\n",
    "    def __init__(self):\n",
    "        super(MKVExporter, self).__init__()\n",
    "\n",
    "    # Function to create a frame from numpy array\n",
    "    def create_frame(self, image_array, stream):\n",
    "        frame = av.VideoFrame.from_ndarray(np.array(image_array), format='rgb24')\n",
    "        frame.pict_type = 'NONE'\n",
    "        frame.time_base = stream.time_base\n",
    "        return frame\n",
    "    \n",
    "    # Function to create a frame from numpy array\n",
    "    def create_frame_depth(self, image_array, stream):\n",
    "        image_array = np.array(image_array)\n",
    "        # if float, convert to uint8\n",
    "        if image_array.dtype == np.float32:\n",
    "            image_array = (image_array * 255).astype(np.uint8)\n",
    "        # if 3 dim, convert to 2 dim\n",
    "        if len(image_array.shape) == 3:\n",
    "            image_array = image_array[:,:,0]\n",
    "        frame = av.VideoFrame.from_ndarray(image_array, format='gray')\n",
    "        frame.pict_type = 'NONE'\n",
    "        frame.time_base = stream.time_base\n",
    "        return frame\n",
    "\n",
    "    def export(self, loader: BaseLoader, output_path: str):\n",
    "        # Create an output container\n",
    "        i = -1\n",
    "        for trajectory in loader:\n",
    "            i += 1\n",
    "            output = av.open(f'{output_path}/output_{i}.mkv', mode='w')\n",
    "            print(f'{output_path}/output_{i}.mkv')\n",
    "            # Define video streams (assuming images are 640x480 RGB)\n",
    "            video_stream_1 = output.add_stream('libx264', rate=1)\n",
    "            video_stream_1.width = 640\n",
    "            video_stream_1.height = 480\n",
    "            video_stream_1.pix_fmt = 'yuv420p'\n",
    "\n",
    "            video_stream_2 = output.add_stream('libx264', rate=1)\n",
    "            video_stream_2.width = 640\n",
    "            video_stream_2.height = 480\n",
    "            video_stream_2.pix_fmt = 'yuv420p'\n",
    "\n",
    "            # Define custom data stream for vectors\n",
    "            depth_stream = output.add_stream('libx264', rate=1)\n",
    "\n",
    "            data_stream = output.add_stream('rawvideo', rate=1)\n",
    "\n",
    "            ts = 0\n",
    "            # convert step data to stream\n",
    "            for step in trajectory:\n",
    "                obesrvation = step[\"observation\"].copy()\n",
    "                obesrvation.pop(\"image\")\n",
    "                obesrvation.pop(\"hand_image\")\n",
    "                obesrvation.pop(\"image_with_depth\")\n",
    "                non_image_data_step = step.copy()\n",
    "                non_image_data_step[\"observation\"] = obesrvation\n",
    "\n",
    "                non_image_data_bytes = pickle.dumps(non_image_data_step)\n",
    "                packet = av.Packet(non_image_data_bytes)\n",
    "                packet.stream = data_stream\n",
    "                packet.pts = ts\n",
    "                output.mux(packet)\n",
    "\n",
    "                image =np.array(step[\"observation\"][\"image\"])\n",
    "                # Create a frame from the numpy array\n",
    "                frame = self.create_frame(image, video_stream_1)\n",
    "                frame.pts = ts\n",
    "                packet = video_stream_1.encode(frame)\n",
    "                \n",
    "                output.mux(packet)\n",
    "\n",
    "                hand_image =np.array(step[\"observation\"][\"hand_image\"])\n",
    "                # Create a frame from the numpy array\n",
    "                frame = self.create_frame(hand_image, video_stream_2)\n",
    "                frame.pts = ts\n",
    "                packet = video_stream_2.encode(frame)\n",
    "                output.mux(packet)\n",
    "\n",
    "                # # Create a frame from the numpy array\n",
    "                frame = self.create_frame_depth(step[\"observation\"][\"image_with_depth\"], depth_stream)\n",
    "                # frame.pts = ts\n",
    "                # Encode the frame\n",
    "                packet = depth_stream.encode(frame)\n",
    "                # Write the packet to the output file\n",
    "                output.mux(packet)\n",
    "\n",
    "                ts += 1\n",
    "\n",
    "            \n",
    "            # Flush the remaining frames\n",
    "            for packet in video_stream_1.encode():\n",
    "                output.mux(packet)\n",
    "\n",
    "            for packet in video_stream_2.encode():\n",
    "                output.mux(packet)\n",
    "\n",
    "            for packet in depth_stream.encode():\n",
    "                output.mux(packet)\n",
    "            output.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89164adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = 1\n",
    "def iterate_dataset(loader: BaseLoader, number_of_samples):\n",
    "    for i, data in enumerate(loader): \n",
    "        if i == number_of_samples:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f192e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "class ParquetExporter():\n",
    "    def __init__(self):\n",
    "        super(ParquetExporter, self).__init__()\n",
    "        self.logger = getLogger(__name__)\n",
    "\n",
    "    def _serialize(self, data):\n",
    "        return pickle.dumps(np.array(data))\n",
    "    def _step_data_to_df(self, step_data):\n",
    "        step = {}\n",
    "        for dataset_key, element in step_data.items():\n",
    "            if dataset_key == \"observation\":\n",
    "                for obs_key, obs_element in element.items():\n",
    "                    step[obs_key] = self._serialize(obs_element)\n",
    "            elif dataset_key == \"action\":\n",
    "                for action_key, action_element in element.items():\n",
    "                    step[action_key] = self._serialize(action_element)\n",
    "            else:\n",
    "                step[dataset_key] = self._serialize(element)\n",
    "        return step\n",
    "    \n",
    "    def export(self, loader: BaseLoader, output_path: str):\n",
    "\n",
    "        for steps_data in loader: \n",
    "\n",
    "            step_data_as_df = [self._step_data_to_df(step_data) for step_data in steps_data]\n",
    "            combined_df = pd.DataFrame(step_data_as_df)\n",
    "            table = pa.Table.from_pandas(combined_df)\n",
    "            pq.write_table(table, output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d56f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ParquetLoader():\n",
    "    def __init__(self, data_path):\n",
    "        super(ParquetLoader, self).__init__()\n",
    "        self.data_dir = data_path\n",
    "        self.logger = getLogger(__name__)\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __iter___(self):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d300199c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I 2024-06-22 20:37:22,328 dataset_info.py:617] Load dataset info from /home/kych/datasets/berkeley_autolab_ur5/0.1.0\n",
      "I 2024-06-22 20:37:22,342 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /home/kych/datasets/berkeley_autolab_ur5/0.1.0.\n",
      "I 2024-06-22 20:37:22,416 logging_logger.py:49] Constructing tf.data.Dataset berkeley_autolab_ur5 for split train[:1], from /home/kych/datasets/berkeley_autolab_ur5/0.1.0\n"
     ]
    }
   ],
   "source": [
    "rtx_loader = RTXLoader(os.path.expanduser(\"~/datasets/berkeley_autolab_ur5/0.1.0\"), split = f'train[:{number_of_samples}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80867561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kych/fog_x/examples/dataloader/mkv_output//output_0.mkv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exporter = MKVExporter()\n",
    "output_path = os.path.expanduser(\"~\") + \"/fog_x/examples/dataloader/mkv_output/\"\n",
    "exporter.export(rtx_loader, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcfc1bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kych/fog_x/examples/dataloader/mkv_output/output_384.mkv\n",
      "288 71 71 71 71\n",
      "/home/kych/fog_x/examples/dataloader/mkv_output/output_375.mkv\n",
      "288 71 71 71 71\n"
     ]
    }
   ],
   "source": [
    "mkv_loader = MKVLoader(output_path)\n",
    "def iterate_dataset(loader: BaseLoader, number_of_samples = 50):\n",
    "    for i, data in enumerate(loader): \n",
    "        if i == number_of_samples:\n",
    "            break\n",
    "iterate_dataset(mkv_loader, number_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0822a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterate_dataset(rtx_loader, number_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30ba398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ParquetExporter().export(rtx_loader, \"output.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
